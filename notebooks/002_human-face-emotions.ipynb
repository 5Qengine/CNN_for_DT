{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13545313,"sourceType":"datasetVersion","datasetId":8602141}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"angry_path=r\"/kaggle/input/human-face-emotions/Data/Angry\"\nfear_path=r\"/kaggle/input/human-face-emotions/Data/Fear\"\nhappy_path=r\"/kaggle/input/human-face-emotions/Data/Happy\"\nsad_path=r\"/kaggle/input/human-face-emotions/Data/Sad\"\nsuprise_path=r\"/kaggle/input/human-face-emotions/Data/Suprise\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"angry=Path(angry_path)\nfear=Path(fear_path)\nhappy=Path(happy_path)\nsad=Path(sad_path)\nsuprise=Path(suprise_path)\n\nangry_row=[]\nfear_row=[]\nhappy_row=[]\nsad_row=[]\nsuprise_row=[]\n\nfor file_path in angry.iterdir():\n    if file_path.is_file():\n        angry_row.append({\"path\":str(file_path),\"label\":\"angry\",\"targets\":0})\n\nfor file_path in fear.iterdir():\n    if file_path.is_file():\n        fear_row.append({\"path\":str(file_path),\"label\":\"fear\",\"targets\":1})\n\nfor file_path in happy.iterdir():\n    if file_path.is_file():\n        happy_row.append({\"path\":str(file_path),\"label\":\"happy\",\"targets\":2})\n\nfor file_path in sad.iterdir():\n    if file_path.is_file():\n        sad_row.append({\"path\":str(file_path),\"label\":\"sad\",\"targets\":3})\n\nfor file_path in suprise.iterdir():\n    if file_path.is_file():\n        suprise_row.append({\"path\":str(file_path),\"label\":\"suprise\",\"targets\":4})\n        \nangry_df=pd.DataFrame(angry_row)\nfear_df=pd.DataFrame(fear_row)\nhappy_df=pd.DataFrame(happy_row)\nsad_df=pd.DataFrame(sad_row)\nsuprise_df=pd.DataFrame(suprise_row)\n\nall_df=pd.concat([angry_df,fear_df,happy_df,sad_df,suprise_df], ignore_index=True)\n\nprint(\"dataframe concat complete\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df,tmp_df=train_test_split(all_df,test_size=0.3,stratify=all_df[\"targets\"])\nval_df,test_df=train_test_split(tmp_df,test_size=0.3,stratify=tmp_df[\"targets\"])\n\nprint(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef visualize_samples(df,nrows=3,ncols=3,nsample=9):\n    df_sample=df.sample(n=min(nsample,len(df)))\n    fig,axs=plt.subplots(nrows,ncols,figsize=(ncols*3, nrows*3))\n    axs=axs.flatten()\n    for ax,(_,row) in zip(axs,df_sample.iterrows()):\n        img=cv2.cvtColor(cv2.imread(row[\"path\"]),cv2.COLOR_BGR2RGB)\n        h,w=img.shape[:2]\n        label=row['label']\n\n        ax.imshow(img)\n        ax.set_title(f\"label : {label}\\n{h} x {w}\",fontsize=10)\n        \n    plt.tight_layout()\n    plt.show()\n    \nvisualize_samples(train_df)        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntr_augment=A.Compose([\n    A.Resize(48,48,p=1),\n    A.HorizontalFlip(p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.10, rotate_limit=10, p=0.5), # 기하학적 변환\n    # shift_limit = 가로세로 방향으로 최대 5%정도 이동 \n    # 얼굴의 위치·크기·각도가 조금 달라도 같은 표정으로 인식하도록 만드는 “현실적인 흔들림” 증강\n    # scale_limit=0.10 : img를 10%범위내에서 확대,축소\n    # rotate_limit=10  10정도 사이에서 회전\n    A.RandomBrightnessContrast(p=0.3), # 광학적변화 - 조명/카메라\n    A.Normalize(mean=(0.5, ),std=(0.5,)),\n    ToTensorV2()\n])\n\nval_augment=A.Compose([\n    A.Resize(48,48,p=1),\n    A.Normalize(mean=(0.5,),std=(0.5,)),  # grayscale로 할꺼니 채널을 하나만!\n    ToTensorV2()\n])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EmotionCustom(torch.utils.data.Dataset):\n    def __init__(self,path,targets,augmentor):\n        super().__init__()\n        self.path=path\n        self.targets=targets\n        self.augmentor=augmentor\n    def __len__(self):\n        return len(self.path)\n    def __getitem__(self,idx):\n        path_idx=self.path[idx]\n        cv2_img=cv2.imread(path_idx,cv2.IMREAD_GRAYSCALE)  # (H,W)\n        if self.augmentor is not None:\n            img=self.augmentor(image=cv2_img)['image']\n        else:\n            raise ValueError(\"augmentor must be need\")\n        targets=torch.tensor(self.targets[idx],dtype=torch.long)\n        return img,targets\n       ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_custom=EmotionCustom(train_df[\"path\"].to_list(),train_df[\"targets\"].to_list(),tr_augment)\nval_custom=EmotionCustom(val_df['path'].to_list(),val_df['targets'].to_list(),val_augment)\ntest_custom=EmotionCustom(test_df['path'].to_list(),test_df['targets'].to_list(),val_augment)\n\ntrain_loader=DataLoader(train_custom,batch_size=128,shuffle=True,num_workers=4,pin_memory=True)\nval_loader=DataLoader(val_custom,batch_size=128,shuffle=False,num_workers=4,pin_memory=True)\ntest_loader=DataLoader(test_custom,batch_size=128,shuffle=False,num_workers=4,pin_memory=True)\nprint('done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k,v=next(iter(train_loader))\nprint(k.shape)\nprint(v.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import resnet34,ResNet34_Weights\n\nnum_classes=5\n\nmodel = resnet34(weights=ResNet34_Weights.DEFAULT)\nmodel.conv1=nn.Conv2d(in_channels=1,out_channels=64, kernel_size=3, stride=1, padding=1, bias=False )\nmodel.maxpool=nn.Identity() # nn.Identity() => x=y  즉, 아무것도 안한다. 구조를 깨지않기 위한 장치. “stem에서만큼은 해상도를 최대한 보존하자”.  maxpool을 주석처리하면 에러. 지워도 에러\n                            # 초반 stem을 바꾼것이라 초반부는 pretrained효과가 없다고 보면 된다. 어쩌면 이게 더 득이 될수도 독이 될수도..\nmodel.fc=nn.Linear(model.fc.in_features, num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom torchmetrics import Accuracy, Recall\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbackbone_params = []\nnew_params = []\n\nfor name, p in model.named_parameters():     # 원래 레이어와 일부레이어의 학습률 차이를 줄때 쓰는 기법\n    if not p.requires_grad:\n        continue\n\n    # conv1 / fc는 새로 만든 파트\n    if name.startswith(\"conv1\") or name.startswith(\"fc\"):\n        new_params.append(p)\n    else:\n        backbone_params.append(p)\n\noptimizer = optim.Adam(\n    [\n        {\"params\": backbone_params, \"lr\": 1e-4},\n        {\"params\": new_params,      \"lr\": 1e-3},\n    ],\n    weight_decay=1e-4\n)\n\nloss_func=nn.CrossEntropyLoss()\nscheduler=ReduceLROnPlateau(optimizer, factor=0.5, patience=4)\nacc_metric=Accuracy(task=\"multiclass\",num_classes=num_classes).to(device)\nrecall_metric=Recall(task=\"multiclass\", num_classes=num_classes ).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_improvement(best_value,value):\n    if value>best_value:\n        return True\n    else:\n        return False\n   \nclass EarlyStop:\n    def __init__(self,early_stop,stop_count):\n        self.early_stop=early_stop\n        self.stop_count=0\n    def stop_logic(self,best_value,value):\n        if is_improvement(best_value,value):\n            self.stop_count=0\n            return False\n        else:\n            if self.stop_count>=self.early_stop:\n                print(\"early_stopped\")\n                return True\n            else:\n                self.stop_count+=1\n                return False\n\nclass Weights_ChkPoint:\n    def __init__(self,path):\n        self.path=path\n        self.count=0\n    def save(self,value):\n        if is_improvement(best_value,value):\n            torch.save(state_dict,f\"{self.count}_{value:.4f}.pt\")\n            self.count+=1\n            return False\n        else:\n            return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List\nfrom dataclasses import dataclass,field\nfrom tqdm import tqdm\n\n@dataclass\nclass History:\n    training_accuracy:List[float]=field(default_factory=list)\n    training_recall:List[float]=field(default_factory=list)\n    training_loss:List[float]=field(default_factory=list)\n    val_accuracy:List[float]=field(default_factory=list)\n    val_recall:List[float]=field(default_factory=list)\n    val_loss:List[float]=field(default_factory=list)\nhistory=History()\n\n\nclass Trainer:\n    def __init__(self,train_loader,val_loader,model,optimizer,loss_func,\n                 scheduler,metric_acc,metric_rec,device,history,mode=\"min\"):\n        self.model=model.to(device)\n        self.train_loader=train_loader\n        self.val_loader=val_loader\n        self.optimizer=optimizer\n        self.loss_func=loss_func\n        self.scheduler=scheduler\n        self.metric_acc=metric_acc.to(device)\n        self.metric_rec=metric_rec.to(device)\n        self.device=device\n        self.history=history\n        if mode==\"max\":\n            self.best_value=float('-inf')\n        else:\n            self.best_value=float('inf')\n\n    def training_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.train()\n        loss_sum=0.0\n        avg_loss=0.0\n        with tqdm(total=len(self.train_loader),desc=f\"training {epoch}\",leave=True) as bar:\n            for batch_idx,(x_train,y_train) in enumerate(self.train_loader):\n                x_train=x_train.to(self.device)\n                y_train=y_train.to(self.device)\n                logits=self.model(x_train)\n                loss=self.loss_func(logits,y_train)\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                loss_sum+=loss.item()\n                avg_loss=loss_sum/(batch_idx+1)\n                preds=logits.argmax(dim=1)   # dim=-1과 같다. (B,12)  1번 dim 즉 행에 대해서\n                self.metric_acc.update(preds, y_train)\n                self.metric_rec.update(preds, y_train)\n                bar.update(1)\n\n                if batch_idx%10==0:\n                    acc=self.metric_acc.compute().item()\n                    recall=self.metric_rec.compute().item()\n                    bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n            return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss  \n\n    def validating_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.eval()\n        loss_sum=0\n        avg_loss=0.0\n        with tqdm(total=len(self.val_loader),desc=f\"validating {epoch}\", leave=True) as bar:\n            with torch.no_grad():\n                for batch_idx,(x_val,y_val) in enumerate(self.val_loader):\n                    x_val=x_val.to(self.device)\n                    y_val=y_val.to(self.device)\n                    logits=self.model(x_val)\n                    loss=self.loss_func(logits,y_val)\n\n                    preds=logits.argmax(dim=-1)\n                    self.metric_acc.update(preds,y_val)\n                    self.metric_rec.update(preds,y_val)\n                    loss_sum+=loss.item()\n                    avg_loss=loss_sum/(batch_idx+1)\n                    bar.update(1)\n                    if batch_idx%10==0:\n                        acc=self.metric_acc.compute().item()\n                        recall=self.metric_rec.compute().item()\n                        bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n                return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss\n\n    \n    def fit(self,epochs,early_stop,path):\n        stop_count=0   \n        for epoch in range(epochs):\n            training_accuracy,training_recall,training_loss=self.training_epoch(epoch)\n            self.history.training_accuracy.append(training_accuracy)\n            self.history.training_recall.append(training_recall)\n            self.history.training_loss.append(training_loss)\n            val_accuracy,val_recall,val_loss=self.validating_epoch(epoch)\n            self.history.val_accuracy.append(val_accuracy)\n            self.history.val_recall.append(val_recall)\n            self.history.val_loss.append(val_loss)\n            self.scheduler.step(val_loss)\n\n            if self.best_value>val_loss:\n                self.best_value=val_loss\n                stop_count=0\n                torch.save(self.model.state_dict(),os.path.join(path,f\"{epoch}_{val_loss}.pt\"))\n            else:\n                stop_count+=1\n                if stop_count>=early_stop:\n                    print(f\"early_stopped. current epoch : {epoch}\")\n                    return self.history\n                    \n        return self.history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path=r\"/kaggle/working/\"\n\nt=Trainer(train_loader,val_loader,model,optimizer,loss_func,scheduler,acc_metric,recall_metric,device,history,mode=\"min\")\nhistory=t.fit(30,10,output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/13_0.3982275021598511.pt\"))\nprint('done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nclass Predictor:\n    def __init__(self, model, test_loader, device):\n        self.model = model.to(device)\n        self.test_loader = test_loader\n        self.device = device\n\n    def pred(self):\n        pred_list, true_list = [], []\n        self.model.eval()\n        with tqdm(total=len(self.test_loader), desc=\"predicting\", leave=True) as bar:\n            with torch.no_grad():\n                for x, y in self.test_loader:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n                    logits = self.model(x)\n                    preds = logits.argmax(dim=-1)\n\n                    pred_list.extend(preds.detach().cpu().tolist())\n                    true_list.extend(y.detach().cpu().tolist())\n                    bar.update(1)\n\n        return pred_list, true_list\n\np=Predictor(model,test_loader,device)\npred,true=p.pred()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\n\ncm = confusion_matrix(true, pred)\nacc = accuracy_score(true, pred)\n\ncm","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}