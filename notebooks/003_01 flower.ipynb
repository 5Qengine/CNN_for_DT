{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ver 1.0\n> cutmix 사용안함\n> \n> seed는 고정\n> \n> classifier학습 - 전체학습 2단계 (전체 학습시는 lr을 1e-5로 함. 1e-4는 과적합이 빨리 올수 있다는 충고반영)\n> \n> BatchNorm freeze는 안함\n>\n> acc : 0.9441233140655106, recall : 0.94255940562606\n>\n> confusion_matrix :\n> [[122   0   3   2   0]\n>  [  1  87   3   1   0]\n>  [  0   1  85   1   1]\n>  [  0   0   0 113   5]\n>  [  0   2   1   8  83]]\n>\n> Grad-CAM / 다른 flowerset으로 테스트 필요\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom albumentations import ToTensorV2\nimport albumentations as A\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim import Adam\nfrom torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy,MulticlassRecall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed: int = 42):\n    random.seed(seed)          # python random\n    np.random.seed(seed)       # numpy\n    torch.manual_seed(seed)    # torch CPU\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n\nbase_path=Path(r\"/kaggle/input/flowers-recognition/flowers\")\npath_list=[]\n\nfor img_path in base_path.rglob(\"*.jpg\"):\n    path_list.append({\"label\":img_path.parent.name,\"path\":img_path})\n\ndf=pd.DataFrame(path_list)\ndf['targets']=pd.factorize(df['label'])[0]\ndf=df.sample(frac=1,random_state=42).reset_index(drop=True)\n\ntrain_df,tmp_df=train_test_split(df,test_size=0.3,stratify=df['label'],random_state=42)\nval_df,test_df=train_test_split(tmp_df,test_size=0.4,stratify=tmp_df['label'],random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_aug=A.Compose([\n    A.RandomResizedCrop(size=(224,224),scale=(0.8,1.0),ratio=(0.9,1.1),p=1),\n    A.HorizontalFlip(p=0.3),\n    A.Affine(scale=(0.9,1.1),rotate=(-15,15),border_mode=cv2.BORDER_REFLECT_101,p=0.3),\n    A.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0.03,p=0.6),\n    A.CoarseDropout(num_holes_range=(1, 1), hole_height_range=(48, 48),\n                    hole_width_range=(48, 48),fill=0,p=0.25)\n])\n\ntr_resnet34=A.Compose([\n    A.RandomResizedCrop(size=(224,224),scale=(0.8,1.0),ratio=(0.9,1.1),p=1),\n    A.HorizontalFlip(p=0.3),\n    A.Affine(scale=(0.9,1.1),rotate=(-15,15),border_mode=cv2.BORDER_REFLECT_101,p=0.3),\n    A.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0.03,p=0.6),\n    A.CoarseDropout(num_holes_range=(1, 1), hole_height_range=(48, 48),\n                    hole_width_range=(48, 48),fill=0,p=0.25),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_resnet34=A.Compose([\n    A.Resize(224,224,p=1),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize(df,nrows=4,ncols=4,augment=None):\n    df=df.sample(min(nrows*ncols,len(df)))\n    fig,axs=plt.subplots(nrows,ncols,figsize=(ncols*3,nrows*3))\n    axs=axs.flatten()\n    for ax,(_,row) in zip(axs,df.iterrows()):\n        img=cv2.imread(row['path'])\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        if augment is not None:\n            img=augment(image=img)['image']\n        H,W=img.shape[:2]\n        label=row['label']\n        ax.imshow(img)\n        ax.set_title(f\"{label}\\n{H}x{W}\")\n    plt.show()\n\nvisualize(df,augment=img_aug)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlowerCustom(Dataset):\n    def __init__(self,path,targets,augment=None):\n        self.path=path\n        self.targets=targets\n        self.augment=augment\n    def __len__(self):\n        return len(self.path)\n    def __getitem__(self,idx):\n        img=cv2.imread(self.path[idx])\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        if self.augment is None:\n            raise ValueError(\"IMG Augment must be need\")\n        img=self.augment(image=img)['image']\n        targets=torch.tensor(self.targets[idx],dtype=torch.long)\n        return img,targets\n\n\ntrain_custom=FlowerCustom(train_df['path'].to_list(),train_df['targets'].to_list(),\n                          augment=tr_resnet34)\nval_custom=FlowerCustom(val_df['path'].to_list(),val_df['targets'].to_list(),\n                        augment=val_resnet34)\ntest_custom=FlowerCustom(test_df['path'].to_list(),test_df['targets'].to_list(),\n                         augment=val_resnet34)\n\ntrain_loader=DataLoader(train_custom,batch_size=32,shuffle=True,num_workers=4,pin_memory=True)\nval_loader=DataLoader(val_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\ntest_loader=DataLoader(test_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import efficientnet_b1,EfficientNet_B1_Weights\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nweights = EfficientNet_B1_Weights.IMAGENET1K_V2\nmodel=efficientnet_b1(weights=weights).to(device)\nmodel.classifier[1]=nn.Linear(in_features=1280, out_features=5, bias=True).to(device)\n\nfor p in model.parameters():\n    p.requires_grad=False\nfor p in model.classifier.parameters():\n    p.requires_grad=True\noptimizer=Adam(model.classifier.parameters(),lr=1e-3,weight_decay=1e-4)\nscheduler=ReduceLROnPlateau(optimizer,factor=0.1,patience=3)\nloss_func=nn.CrossEntropyLoss()\nmetric_rec=MulticlassRecall(num_classes=5,average=\"macro\").to(device)\nmetric_acc=MulticlassAccuracy(num_classes=5).to(device)\nmetric_f1=MulticlassF1Score(num_classes=5,average='macro').to(device)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List\nfrom dataclasses import dataclass,field\nfrom tqdm import tqdm\n\n@dataclass\nclass History:\n    training_accuracy:List[float]=field(default_factory=list)\n    training_recall:List[float]=field(default_factory=list)\n    training_loss:List[float]=field(default_factory=list)\n    val_accuracy:List[float]=field(default_factory=list)\n    val_recall:List[float]=field(default_factory=list)\n    val_loss:List[float]=field(default_factory=list)\nhistory=History()\n\n\nclass Trainer:\n    def __init__(self,train_loader,val_loader,model,optimizer,loss_func,\n                 scheduler,metric_acc,metric_rec,device,history,mode=\"min\"):\n        self.model=model\n        self.train_loader=train_loader\n        self.val_loader=val_loader\n        self.optimizer=optimizer\n        self.loss_func=loss_func\n        self.scheduler=scheduler\n        self.metric_acc=metric_acc\n        self.metric_rec=metric_rec\n        self.device=device\n        self.history=history\n        if mode==\"max\":\n            self.best_value=float('-inf')\n        else:\n            self.best_value=float('inf')\n\n    def training_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.train()\n        loss_sum=0.0\n        avg_loss=0.0\n        with tqdm(total=len(self.train_loader),desc=f\"training {epoch}\",leave=True) as bar:\n            for batch_idx,(x_train,y_train) in enumerate(self.train_loader):\n                x_train=x_train.to(self.device)\n                y_train=y_train.to(self.device)\n                logits=self.model(x_train)\n                loss=self.loss_func(logits,y_train)\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                loss_sum+=loss.item()\n                avg_loss=loss_sum/(batch_idx+1)\n                preds=logits.argmax(dim=1)   # dim=-1과 같다. (B,12)  1번 dim 즉 행에 대해서\n                self.metric_acc.update(preds, y_train)\n                self.metric_rec.update(preds, y_train)\n                bar.update(1)\n\n                if batch_idx%10==0:\n                    acc=self.metric_acc.compute().item()\n                    recall=self.metric_rec.compute().item()\n                    bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n            return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss  \n\n    def validating_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.eval()\n        loss_sum=0\n        avg_loss=0.0\n        with tqdm(total=len(self.val_loader),desc=f\"validating {epoch}\", leave=True) as bar:\n            with torch.no_grad():\n                for batch_idx,(x_val,y_val) in enumerate(self.val_loader):\n                    x_val=x_val.to(self.device)\n                    y_val=y_val.to(self.device)\n                    logits=self.model(x_val)\n                    loss=self.loss_func(logits,y_val)\n\n                    preds=logits.argmax(dim=-1)\n                    self.metric_acc.update(preds,y_val)\n                    self.metric_rec.update(preds,y_val)\n                    loss_sum+=loss.item()\n                    avg_loss=loss_sum/(batch_idx+1)\n                    bar.update(1)\n                    if batch_idx%10==0:\n                        acc=self.metric_acc.compute().item()\n                        recall=self.metric_rec.compute().item()\n                        bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n                return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss\n\n    \n    def fit(self,epochs,early_stop,path):\n        stop_count=0   \n        for epoch in range(epochs):\n            training_accuracy,training_recall,training_loss=self.training_epoch(epoch)\n            self.history.training_accuracy.append(training_accuracy)\n            self.history.training_recall.append(training_recall)\n            self.history.training_loss.append(training_loss)\n            val_accuracy,val_recall,val_loss=self.validating_epoch(epoch)\n            self.history.val_accuracy.append(val_accuracy)\n            self.history.val_recall.append(val_recall)\n            self.history.val_loss.append(val_loss)\n            self.scheduler.step(val_loss)   # scheduler는 early_stop >= scheduler.patience + 1정도가 안정적. ex)scheduler patience = 3이면 early_stop = 5\n            \n            if self.best_value>val_loss:\n                self.best_value=val_loss\n                stop_count=0\n                torch.save(self.model.state_dict(),os.path.join(path,f\"{epoch}_{val_loss}.pt\"))\n            else:\n                stop_count+=1\n                if stop_count>=early_stop:\n                    print(f\"early_stopped. current epoch : {epoch}\")\n                    return self.history\n                    \n        return self.history\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path=r\"/kaggle/working/\"\n\nt=Trainer(train_loader,val_loader,model,optimizer,loss_func,scheduler,metric_acc,metric_rec,device,history,mode=\"min\")\nhistory=t.fit(5,2,output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbest_param=torch.load(r\"/kaggle/working/4_0.31886756539344785.pt\")\nmodel.load_state_dict(best_param)\nfor p in model.parameters():\n    p.requires_grad=True\noptimizer=Adam(model.parameters(),lr=1e-5,weight_decay=1e-4)    # 1e-4는 3,000장에선 다소 공격적\nt1=Trainer(train_loader,val_loader,model,optimizer,loss_func,scheduler,metric_acc,metric_rec,device,history,mode=\"min\")\nhistory=t1.fit(20,5,output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_param=torch.load(r\"/kaggle/working/19_0.1560720184445381.pt\")\nmodel.load_state_dict(final_param)\n\nclass Predict:\n    def __init__(self,model,test_loader,device):\n        self.model=model\n        self.test_loader=test_loader\n        self.actual_list=[]\n        self.pred_list=[]\n        self.device=device\n    def predict(self):\n        self.model.eval()\n        with tqdm(total=len(self.test_loader),desc=f\"predicting\",leave=True) as bar:\n            for x_test,y_test in self.test_loader:\n                x_test=x_test.to(self.device)\n                y_test=y_test.to(self.device)\n                self.actual_list.extend(y_test.cpu().detach().numpy())\n                logits=self.model(x_test)\n                preds=torch.argmax(logits,dim=-1)\n                self.pred_list.extend(preds.cpu().detach().numpy())\n                bar.update(1)\n            return self.actual_list,self.pred_list    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p=Predict(model,test_loader,device)\nactual_list,pred_list=p.predict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score\n\n\ncm=confusion_matrix(actual_list,pred_list)\nacc = accuracy_score(actual_list, pred_list)\nrecall = recall_score(actual_list, pred_list, average=\"macro\")\n\nprint(cm)\nprint(f\"acc : {acc}\")\nprint(f\"recall : {recall}\")\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}