{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":211650,"sourceType":"datasetVersion","datasetId":91319},{"sourceId":14331372,"sourceType":"datasetVersion","datasetId":9149654}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### ì‘ë¬¼í´ë˜ìŠ¤\n> Common wheat, Maize, Sugar beet\n\n> ë‚˜ë¨¸ì§€ëŠ” ì¡ì´ˆ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport albumentations as A\nfrom albumentations import ToTensorV2\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom dataclasses import dataclass,field","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows = []\n\nmain_path=Path(\"/kaggle/input/v2-plant-seedlings-dataset\")\noutput_path=r\"/kaggle/working/\"\nfor class_dir in main_path.iterdir():\n    if class_dir.is_dir():\n        label=class_dir.name\n        for img_path in class_dir.glob(\"*\"):\n            if img_path.suffix.lower() == \".png\":\n                rows.append({\"path\":str(img_path),\"label\":label})\n\ndf=pd.DataFrame(rows)\ndf.tail()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = sorted(df[\"label\"].unique())\nmapping = {c:i for i,c in enumerate(classes)}\ndf[\"targets\"]=df['label'].map(mapping)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df,tmp_df=train_test_split(df,test_size=0.3,stratify=df[\"targets\"])\nval_df,test_df=train_test_split(tmp_df,test_size=0.3,stratify=tmp_df[\"targets\"])\n\nprint(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport math\n\ndef visualize_samples(df, n_samples=24, ncols=6):\n    \"\"\"\n    df: path, targets ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame\n    n_samples: ì‹œê°í™”í•  ì´ë¯¸ì§€ ê°œìˆ˜\n    ncols: í•œ ì¤„ì— ë³´ì—¬ì¤„ ì´ë¯¸ì§€ ìˆ˜\n    \"\"\"\n    df_sample = df.sample(n=min(n_samples, len(df)), random_state=12)\n\n    nrows = math.ceil(len(df_sample) / ncols)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 3, nrows * 3))\n    axes = axes.flatten()\n\n    for ax, (_, row) in zip(axes, df_sample.iterrows()):\n        img = Image.open(row[\"path\"]).convert(\"RGB\")\n        w, h = img.size\n        target = row[\"targets\"]\n\n        ax.imshow(img)\n        ax.set_title(f\"target: {target}\\n{h} x {w}\", fontsize=10)\n        ax.axis(\"off\")\n\n    # ë‚¨ëŠ” subplot ì œê±°\n    for ax in axes[len(df_sample):]:\n        ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_samples(train_df, n_samples=30, ncols=6)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_augmentor=A.Compose([\n    A.Resize(224,224,p=1),\n    A.HorizontalFlip(p=0.1),\n    A.VerticalFlip(p=0.1),\n    A.HueSaturationValue(\n    hue_shift_limit=5,     # ğŸ”´ ë§¤ìš° ì¤‘ìš”: ë…¹ìƒ‰ê³„ì—´ì—ì„œ Hueë¥¼ ì„¸ê²Œ ì£¼ë©´ ì¬ì•™ì  ê²°ê³¼ì´ˆë˜. ë…¹ìƒ‰ â†’ â€œë…¹ìƒ‰ ë²”ìœ„ ì•ˆì—ì„œë§Œâ€ í”ë“¤ë¦¼\n    sat_shift_limit=20,\n    val_shift_limit=15,\n    p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\nval_augmentor=A.Compose([\n    A.Resize(224,224,p=1),\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PlantCustom(torch.utils.data.Dataset):\n    def __init__(self,path,targets,augmentor=None):\n        self.path=path\n        self.targets=targets\n        self.augmentor=augmentor\n    def __len__(self):\n        return len(self.path)\n    def __getitem__(self,idx):\n        raw_path=self.path[idx]\n        raw_img=cv2.cvtColor(cv2.imread(raw_path),cv2.COLOR_BGR2RGB)\n        if self.augmentor is not None:\n            img=self.augmentor(image=raw_img)['image']\n        else:\n            raise FileError(\"augment logic must be need\")\n        targets=torch.tensor(self.targets[idx],dtype=torch.long)\n        return img,targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_custom=PlantCustom(train_df[\"path\"].to_list(), train_df[\"targets\"].to_list(), augmentor=tr_augmentor)\nval_custom=PlantCustom(val_df[\"path\"].to_list(),val_df[\"targets\"].to_list(),augmentor=val_augmentor)\ntest_custom=PlantCustom(test_df[\"path\"].to_list(),test_df[\"targets\"].to_list(),augmentor=val_augmentor)\n\ntrain_loader=DataLoader(train_custom,batch_size=32,shuffle=True,num_workers=4,pin_memory=True)\nval_loader=DataLoader(val_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\ntest_loader=DataLoader(test_custom,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n\nprint(\"ok\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k,v=next(iter(train_loader))\nprint(k.shape)\nprint(v.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import resnet50, ResNet50_Weights\nfrom torchinfo import summary\nfrom torchmetrics import Accuracy\nfrom torchmetrics.classification import MulticlassRecall\n\nmodel = resnet50(weights=ResNet50_Weights.DEFAULT)\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 300),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.2),\n    nn.Linear(300, 100),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.2),\n    nn.Linear(100, 12),\n)\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\noptimizer=torch.optim.Adam(model.parameters(),lr=0.001)\nscheduler=ReduceLROnPlateau(optimizer,factor=0.5,patience=3,threshold=0.001)\nmetric_acc=Accuracy(task=\"multiclass\",num_classes=12)\nmetric_rec=MulticlassRecall(num_classes=12,average='macro')\nloss_func=nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# summary(model,input_size=(32,3,224,224),depth=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef is_improvement(best_value,value):\n    if value>best_value:\n        return True\n    else:\n        return False\n   \nclass EarlyStop:\n    def __init__(self,early_stop,stop_count):\n        self.early_stop=early_stop\n        self.stop_count=0\n    def stop_logic(self,best_value,value):\n        if is_improvement(best_value,value):\n            self.stop_count=0\n            return False\n        else:\n            if self.stop_count>=self.early_stop:\n                print(\"early_stopped\")\n                return True\n            else:\n                self.stop_count+=1\n                return False\n\nclass Weights_ChkPoint:\n    def __init__(self,path):\n        self.path=path\n        self.count=0\n    def save(self,value):\n        if is_improvement(best_value,value):\n            torch.save(state_dict,f\"{self.count}_{value:.4f}.pt\")\n            self.count+=1\n            return False\n        else:\n            return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List\n\n@dataclass\nclass History:\n    training_accuracy:List[float]=field(default_factory=list)\n    training_recall:List[float]=field(default_factory=list)\n    training_loss:List[float]=field(default_factory=list)\n    val_accuracy:List[float]=field(default_factory=list)\n    val_recall:List[float]=field(default_factory=list)\n    val_loss:List[float]=field(default_factory=list)\nhistory=History()\n\n\nclass Trainer:\n    def __init__(self,train_loader,val_loader,model,optimizer,loss_func,\n                 scheduler,metric_acc,metric_rec,device,history,mode=\"min\"):\n        self.model=model.to(device)\n        self.train_loader=train_loader\n        self.val_loader=val_loader\n        self.optimizer=optimizer\n        self.loss_func=loss_func\n        self.scheduler=scheduler\n        self.metric_acc=metric_acc.to(device)\n        self.metric_rec=metric_rec.to(device)\n        self.device=device\n        self.history=history\n        if mode==\"max\":\n            self.best_value=float('-inf')\n        else:\n            self.best_value=float('inf')\n\n    def training_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.train()\n        loss_sum=0.0\n        avg_loss=0.0\n        with tqdm(total=len(self.train_loader),desc=f\"training {epoch}\",leave=True) as bar:\n            for batch_idx,(x_train,y_train) in enumerate(self.train_loader):\n                x_train=x_train.to(self.device)\n                y_train=y_train.to(self.device)\n                logits=self.model(x_train)\n                loss=self.loss_func(logits,y_train)\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                loss_sum+=loss.item()\n                avg_loss=loss_sum/(batch_idx+1)\n                preds=logits.argmax(dim=1)   # dim=-1ê³¼ ê°™ë‹¤. (B,12)  1ë²ˆ dim ì¦‰ í–‰ì— ëŒ€í•´ì„œ\n                self.metric_acc.update(preds, y_train)\n                self.metric_rec.update(preds, y_train)\n                bar.update(1)\n\n                if batch_idx%10==0:\n                    acc=self.metric_acc.compute().item()\n                    recall=self.metric_rec.compute().item()\n                    bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n            return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss  \n\n    def validating_epoch(self,epoch):\n        self.metric_acc.reset()\n        self.metric_rec.reset()\n        self.model.eval()\n        loss_sum=0\n        avg_loss=0.0\n        with tqdm(total=len(self.val_loader),desc=f\"validating {epoch}\", leave=True) as bar:\n            with torch.no_grad():\n                for batch_idx,(x_val,y_val) in enumerate(self.val_loader):\n                    x_val=x_val.to(self.device)\n                    y_val=y_val.to(self.device)\n                    logits=self.model(x_val)\n                    loss=self.loss_func(logits,y_val)\n\n                    preds=logits.argmax(dim=-1)\n                    self.metric_acc.update(preds,y_val)\n                    self.metric_rec.update(preds,y_val)\n                    loss_sum+=loss.item()\n                    avg_loss=loss_sum/(batch_idx+1)\n                    bar.update(1)\n                    if batch_idx%10==0:\n                        acc=self.metric_acc.compute().item()\n                        recall=self.metric_rec.compute().item()\n                        bar.set_postfix({\"acc\": acc, \"recall\":recall, \"loss\":avg_loss,\"epoch\":epoch})\n                return self.metric_acc.compute().item(), self.metric_rec.compute().item(),avg_loss\n\n    \n    def fit(self,epochs,early_stop,path):\n        stop_count=0   \n        for epoch in range(epochs):\n            training_accuracy,training_recall,training_loss=self.training_epoch(epoch)\n            self.history.training_accuracy.append(training_accuracy)\n            self.history.training_recall.append(training_recall)\n            self.history.training_loss.append(training_loss)\n            val_accuracy,val_recall,val_loss=self.validating_epoch(epoch)\n            self.history.val_accuracy.append(val_accuracy)\n            self.history.val_recall.append(val_recall)\n            self.history.val_loss.append(val_loss)\n            self.scheduler.step(val_loss)\n\n            if self.best_value>val_loss:\n                self.best_value=val_loss\n                stop_count=0\n                torch.save(self.model.state_dict(),os.path.join(path,f\"{epoch}_{val_loss}.pt\"))\n            else:\n                stop_count+=1\n                if stop_count>=early_stop:\n                    print(f\"early_stopped. current epoch : {epoch}\")\n                    return self.history\n                    \n        return self.history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t=Trainer(train_loader,val_loader,model,optimizer,loss_func,scheduler,metric_acc,metric_rec,device,history,mode=\"min\")\nhistory=t.fit(30,10,output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/resnet50-v2plant-seedling-dataset-weights/23_0.14800940760029038.pt\"))\nprint('done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nclass Predictor:\n    def __init__(self, model, test_loader, device):\n        self.model = model.to(device)\n        self.test_loader = test_loader\n        self.device = device\n\n    def pred(self):\n        pred_list, true_list = [], []\n        self.model.eval()\n        with tqdm(total=len(self.test_loader), desc=\"predicting\", leave=True) as bar:\n            with torch.no_grad():\n                for x, y in self.test_loader:\n                    x = x.to(self.device)\n                    y = y.to(self.device)\n                    logits = self.model(x)\n                    preds = logits.argmax(dim=-1)\n\n                    pred_list.extend(preds.detach().cpu().tolist())\n                    true_list.extend(y.detach().cpu().tolist())\n                    bar.update(1)\n\n        return pred_list, true_list\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p=Predictor(model,test_loader,device)\npred,true=p.pred()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\n\ncm = confusion_matrix(true, pred)\nacc = accuracy_score(true, pred)\nprint(acc, cm.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['preds']=pred   # ë‹¹ì—°íˆ dfì˜ í–‰ì˜ ê°¯ìˆ˜ì™€ ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œì˜ ìˆ˜ê°€ ê°™ì•„ì•¼ ì„±ë¦½\ntest_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(mapping)\npreds_map={}\nfor k,v in mapping.items():\n    preds_map[v]=k\n\ntest_df['preds_label']=test_df['preds'].map(preds_map)\ntest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wrong_df=test_df[test_df[\"targets\"]!=test_df[\"preds\"]]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef show_wrong_predictions_overlay(\n    df,\n    max_images=16,\n    cols=4,\n    hspace=0.45,   # â¬…ï¸ í–‰ ê°„ê²©\n    wspace=0.15    # â¬…ï¸ ì—´ ê°„ê²©\n):\n    # 1ï¸âƒ£ ì˜¤ë‹µë§Œ í•„í„°ë§\n    df = df[df[\"targets\"] != df[\"preds\"]].head(max_images)\n\n    rows = (len(df) + cols - 1) // cols\n    fig = plt.figure(figsize=(cols * 4, rows * 4))\n\n    for idx, (_, row) in enumerate(df.iterrows()):\n        img = Image.open(row[\"path\"]).convert(\"RGB\")\n\n        ax = fig.add_subplot(rows, cols, idx + 1)\n        ax.imshow(img)\n        ax.axis(\"off\")\n\n        # ìƒë‹¨: ì •ë‹µ ë¼ë²¨\n        ax.text(\n            0.5, 1.05,\n            f\"GT : {row['label']}\",\n            transform=ax.transAxes,\n            ha=\"center\", va=\"bottom\",\n            fontsize=11,\n            color=\"green\",\n            fontweight=\"bold\"\n        )\n\n        # í•˜ë‹¨: ì˜ˆì¸¡ ë¼ë²¨\n        ax.text(\n            0.5, -0.12,\n            f\"PRED : {row['preds_label']}\",\n            transform=ax.transAxes,\n            ha=\"center\", va=\"top\",\n            fontsize=10,\n            color=\"red\",\n            fontweight=\"bold\"\n        )\n\n    # 2ï¸âƒ£ subplot ê°„ê²© ì§ì ‘ ì œì–´ (í•µì‹¬)\n    plt.subplots_adjust(hspace=hspace, wspace=wspace)\n    plt.show()\n\n\nshow_wrong_predictions_overlay(\n    test_df,\n    max_images=12,\n    cols=5,\n    hspace=0.8,   # í–‰ ê°„ê²© ë” ë„“ê²Œ\n    wspace=0.2\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ìœ„ ê²°ê³¼ë§Œ ë³´ë©´ Loose Silky-bentì™€ Black-grassê°„ í˜¼ë™ì´ ì œì¼ í¬ë‹¤ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤. ","metadata":{}}]}